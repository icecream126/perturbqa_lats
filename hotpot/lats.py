"""
LATS (Language Agent Tree Search) ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„

ì´ ëª¨ë“ˆì€ MCTS(Monte Carlo Tree Search) ê¸°ë°˜ì˜ ì–¸ì–´ ì—ì´ì „íŠ¸ íŠ¸ë¦¬ íƒìƒ‰ ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
ì§ˆë¬¸ ë‹µë³€ ì‘ì—…ì—ì„œ LLMì„ ì‚¬ìš©í•˜ì—¬ Thought-Action-Observation ì‹œí€€ìŠ¤ë¥¼ íƒìƒ‰í•˜ê³  ìµœì ì˜ ë‹µì„ ì°¾ìŠµë‹ˆë‹¤.

ì£¼ìš” êµ¬ì„± ìš”ì†Œ:
- Node: íƒìƒ‰ íŠ¸ë¦¬ì˜ ë…¸ë“œë¥¼ ë‚˜íƒ€ë‚´ëŠ” í´ë˜ìŠ¤
- lats_search: ë©”ì¸ ê²€ìƒ‰ í•¨ìˆ˜ (Selection, Expansion, Simulation, Backpropagation ë‹¨ê³„ ìˆ˜í–‰)
- get_samples: LLMì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ì•¡ì…˜ í›„ë³´ ìƒì„±
- get_value/get_values: ë…¸ë“œì˜ ê°€ì¹˜ í‰ê°€
- select_node: UCTë¥¼ ì‚¬ìš©í•œ ë…¸ë“œ ì„ íƒ
- expand_node: ë…¸ë“œ í™•ì¥
- rollout: ì‹œë®¬ë ˆì´ì…˜ ìˆ˜í–‰
- backpropagate: ê²°ê³¼ ì—­ì „íŒŒ
"""

import itertools
import numpy as np
from functools import partial
from models import gpt as _gpt_base
import wikienv, wrappers

# ì „ì—­ gpt í•¨ìˆ˜ - lats_searchì—ì„œ partialë¡œ ì„¤ì •ë¨
# ë‹¤ë¥¸ ëª¨ë“ˆ(perturbqa.py ë“±)ì—ì„œë„ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡ ëª¨ë“ˆ ë ˆë²¨ ë³€ìˆ˜ë¡œ ì„¤ì •
gpt = _gpt_base
import requests
import logging
import random
import os

# í™˜ê²½ì€ ì§€ì—° ì´ˆê¸°í™”ë˜ë¯€ë¡œ CLI ì¸ìê°€ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë®ì–´ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# ì „ì—­ ë³€ìˆ˜ë¡œ ì €ì¥í•˜ì—¬ ì—¬ëŸ¬ ë²ˆ ì´ˆê¸°í™”ë˜ëŠ” ê²ƒì„ ë°©ì§€í•©ë‹ˆë‹¤.
env = None


def _ensure_env(args):
    """
    í™˜ê²½(Environment)ì„ í•œ ë²ˆë§Œ ìƒì„±í•©ë‹ˆë‹¤. CLI ì¸ìë¥¼ ìš°ì„  ì‚¬ìš©í•˜ê³ , ì—†ìœ¼ë©´ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    
    ì´ í•¨ìˆ˜ëŠ” ì‹±ê¸€í†¤ íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ í™˜ê²½ì„ í•œ ë²ˆë§Œ ì´ˆê¸°í™”í•˜ê³  ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.
    ë°ì´í„°ì…‹ íƒ€ì…ì— ë”°ë¼ HotPotQA ë˜ëŠ” PerturbQA í™˜ê²½ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        args: ëª…ë ¹ì¤„ ì¸ì ê°ì²´ (dataset_type, perturbqa_data_dir ë“±ì„ í¬í•¨)
    
    Returns:
        ì´ˆê¸°í™”ëœ í™˜ê²½ ê°ì²´ (LoggingWrapperë¡œ ê°ì‹¸ì§„ í™˜ê²½)
    """
    global env
    if env is not None:
        return env

    dataset_type = getattr(args, "dataset_type", None) or os.getenv("DATASET_TYPE", "hotpotqa")
    base_env = wikienv.WikiEnv()

    
    if dataset_type == "perturbqa":
        sorted_genes_dir = getattr(args, "perturbqa_data_dir", None) or os.getenv("PERTURBQA_DATA_DIR")
        if sorted_genes_dir is None:
            raise ValueError(
                "PERTURBQA_DATA_DIR must be provided (env var or --perturbqa_data_dir) for perturbqa dataset"
            )
        base_env = wrappers.PerturbQAWrapper(base_env, sorted_genes_dir=sorted_genes_dir)
    else:
        # Default to HotPotQA for backward compatibility
        base_env = wrappers.HotPotQAWrapper(base_env, split="train")

    env = wrappers.LoggingWrapper(base_env)
    return env

# ì „ì—­ ë³€ìˆ˜: ì‹¤íŒ¨í•œ ê¶¤ì ê³¼ ìê°€ ë°˜ì„± ì •ë³´ë¥¼ ì €ì¥
# - reflection_map: ì‹¤íŒ¨í•œ ê¶¤ì ì— ëŒ€í•œ LLMì˜ ìê°€ ë°˜ì„± ê²°ê³¼ë¥¼ ì €ì¥
# - failed_trajectories: ë³´ìƒì´ 0ì¸ ì¢…ë£Œ ë…¸ë“œë“¤ì˜ ê¶¤ì ì„ ì €ì¥ (í•™ìŠµì— í™œìš©)
global reflection_map
global failed_trajectories
reflection_map = []  # ìê°€ ë°˜ì„± ë§µ: ì‹¤íŒ¨í•œ ê¶¤ì ì— ëŒ€í•œ ë°˜ì„± ì •ë³´
failed_trajectories = []  # ì‹¤íŒ¨í•œ ê¶¤ì  ë¦¬ìŠ¤íŠ¸: {'trajectory': str, 'final_answer': str} í˜•ì‹

def step(env, action):
    """
    í™˜ê²½ì—ì„œ ì•¡ì…˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤. íƒ€ì„ì•„ì›ƒ ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´ ìµœëŒ€ 10ë²ˆê¹Œì§€ ì¬ì‹œë„í•©ë‹ˆë‹¤.
    
    ë„¤íŠ¸ì›Œí¬ ìš”ì²­ì´ ì‹¤íŒ¨í•˜ê±°ë‚˜ íƒ€ì„ì•„ì›ƒì´ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, 
    ì•ˆì •ì„±ì„ ìœ„í•´ ì¬ì‹œë„ ë¡œì§ì„ í¬í•¨í•©ë‹ˆë‹¤.
    
    Args:
        env: ì‹¤í–‰í•  í™˜ê²½ ê°ì²´
        action: ì‹¤í–‰í•  ì•¡ì…˜ (ì˜ˆ: "search[entity]", "lookup[keyword]", "finish[answer]")
    
    Returns:
        (observation, reward, done, info) íŠœí”Œ
        - observation: ì•¡ì…˜ ì‹¤í–‰ í›„ ê´€ì°°ëœ ê²°ê³¼
        - reward: ë³´ìƒ ê°’ (0 ë˜ëŠ” 1)
        - done: ì—í”¼ì†Œë“œ ì¢…ë£Œ ì—¬ë¶€
        - info: ì¶”ê°€ ì •ë³´ (ì •ë‹µ, í‰ê°€ ë©”íŠ¸ë¦­ ë“±)
    """
    attempts = 0
    while attempts < 10:
        try:
            return env.step(action)
        except requests.exceptions.Timeout:
            attempts += 1

def get_value(task, x, y, n_evaluate_sample, cache_value=True):
    """
    ë‹¨ì¼ ë…¸ë“œ(ë¶€ë¶„ ê¶¤ì )ì˜ ê°€ì¹˜ë¥¼ LLMì„ ì‚¬ìš©í•˜ì—¬ í‰ê°€í•©ë‹ˆë‹¤.
    
    ì´ í•¨ìˆ˜ëŠ” í˜„ì¬ê¹Œì§€ì˜ ì¶”ë¡  ê¶¤ì (y)ì´ ì–¼ë§ˆë‚˜ ì˜¬ë°”ë¥¸ì§€ í‰ê°€í•©ë‹ˆë‹¤.
    ì‹¤íŒ¨í•œ ê¶¤ì ë“¤ê³¼ ë°˜ì„±(reflection) ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ ë” ì •í™•í•œ í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    ìºì‹±ì„ í†µí•´ ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ë°˜ë³µ í‰ê°€ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
    
    Args:
        task: Task ê°ì²´ (value_prompt_wrap, value_outputs_unwrap ë©”ì„œë“œ í¬í•¨)
        x: ì›ë³¸ ì§ˆë¬¸ ë˜ëŠ” í”„ë¡¬í”„íŠ¸
        y: í‰ê°€í•  ë¶€ë¶„ ê¶¤ì  (í˜„ì¬ê¹Œì§€ì˜ Thought, Action, Observation ì‹œí€€ìŠ¤)
        n_evaluate_sample: LLMìœ¼ë¡œë¶€í„° ìƒì„±í•  í‰ê°€ ìƒ˜í”Œ ìˆ˜ (ì—¬ëŸ¬ ìƒ˜í”Œì˜ í‰ê·  ì‚¬ìš©)
        cache_value: ê°€ì¹˜ ìºì‹± ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
    
    Returns:
        float: ë…¸ë“œì˜ ê°€ì¹˜ (0.0 ~ 1.0 ì‚¬ì´ì˜ ê°’, ë˜ëŠ” -1.0 if í‰ê°€ ì‹¤íŒ¨)
    """
    global reflection_map
    global failed_trajectories
    
    unique_trajectories = get_unique_trajectories(failed_trajectories)
    value_prompt = task.value_prompt_wrap(x, y, unique_trajectories, reflection_map)
    logging.info(f"Current: {x}")
    logging.info(f"Current: {y}")
    if cache_value and value_prompt in task.value_cache:
        return task.value_cache[value_prompt]
    logging.info(f"VALUE PROMPT: {value_prompt}")
    # max_tokensë¥¼ ì¶©ë¶„íˆ í¬ê²Œ ì„¤ì •í•˜ì—¬ ì‘ë‹µì´ ì˜ë¦¬ì§€ ì•Šë„ë¡ í•¨
    # ê°€ì¹˜ í‰ê°€ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì§§ì€ ì‘ë‹µì´ì§€ë§Œ, ì•ˆì „ì„ ìœ„í•´ 500 í† í° ì„¤ì •
    value_outputs = gpt(value_prompt, n=n_evaluate_sample, stop=None, max_tokens=500)
    logging.info(f"VALUE OUTPUTS: {value_outputs}")
    value = task.value_outputs_unwrap(value_outputs)
    logging.info(f"VALUES: {value}")
    if cache_value:
        task.value_cache[value_prompt] = value
    return value

def get_values(task, x, ys, n_evaluate_sample, cache_value=True):
    """
    ì—¬ëŸ¬ ë…¸ë“œ(ë¶€ë¶„ ê¶¤ì ë“¤)ì˜ ê°€ì¹˜ë¥¼ ì¼ê´„ í‰ê°€í•©ë‹ˆë‹¤.
    
    ì—¬ëŸ¬ í›„ë³´ ê¶¤ì ë“¤ì— ëŒ€í•´ ê°ê°ì˜ ê°€ì¹˜ë¥¼ í‰ê°€í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì¤‘ë³µëœ ê¶¤ì ì— ëŒ€í•´ì„œëŠ” ì¬í‰ê°€ë¥¼ í”¼í•˜ê¸° ìœ„í•´ ë¡œì»¬ ìºì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    
    Args:
        task: Task ê°ì²´
        x: ì›ë³¸ ì§ˆë¬¸ ë˜ëŠ” í”„ë¡¬í”„íŠ¸
        ys: í‰ê°€í•  ë¶€ë¶„ ê¶¤ì ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
        n_evaluate_sample: LLMìœ¼ë¡œë¶€í„° ìƒì„±í•  í‰ê°€ ìƒ˜í”Œ ìˆ˜
        cache_value: ê°€ì¹˜ ìºì‹± ì‚¬ìš© ì—¬ë¶€
    
    Returns:
        list: ê° ê¶¤ì ì— ëŒ€í•œ ê°€ì¹˜ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸ (ê¸¸ì´ëŠ” ysì™€ ë™ì¼)
    """
    values = []
    local_value_cache = {}
    for y in ys:  # each partial output
        if y in local_value_cache:  # avoid duplicate candidates
            value = 0
        else:    
            value = get_value(task, x, y, n_evaluate_sample, cache_value=cache_value)
            local_value_cache[y] = value
        values.append(value)
    return values

def get_samples(task, x, y, n_generate_sample, prompt_sample, stop):
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ë‹¨ê³„ì˜ ì•¡ì…˜ í›„ë³´ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    í˜„ì¬ ìƒíƒœ(x, y)ì—ì„œ ë‹¤ìŒì— ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” Thoughtì™€ Actionì„ ìƒì„±í•©ë‹ˆë‹¤.
    ì‹¤íŒ¨í•œ ê¶¤ì ì´ ìˆìœ¼ë©´ ìê°€ ë°˜ì„±(self-reflection)ì„ ìƒì„±í•˜ì—¬ ë” ë‚˜ì€ ì•¡ì…˜ì„ ìƒì„±í•˜ë„ë¡ í•©ë‹ˆë‹¤.
    í”„ë¡¬í”„íŠ¸ íƒ€ì…ì— ë”°ë¼ standard ë˜ëŠ” chain-of-thought (cot) ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    
    Args:
        task: Task ê°ì²´ (standard_prompt_wrap, cot_prompt_wrap ë©”ì„œë“œ í¬í•¨)
        x: ì›ë³¸ ì§ˆë¬¸ ë˜ëŠ” í”„ë¡¬í”„íŠ¸
        y: í˜„ì¬ê¹Œì§€ì˜ ê¶¤ì  (ë‹¤ìŒ ì•¡ì…˜ì„ ìƒì„±í•  ê¸°ì¤€ì )
        n_generate_sample: ìƒì„±í•  ì•¡ì…˜ í›„ë³´ ìˆ˜
        prompt_sample: í”„ë¡¬í”„íŠ¸ íƒ€ì… ('standard' ë˜ëŠ” 'cot')
        stop: ìƒì„± ì¤‘ë‹¨ í† í° (ì˜ˆ: "Observation")
    
    Returns:
        list: ìƒì„±ëœ ì•¡ì…˜ í›„ë³´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ (ê°ê°ì€ yì— ì´ì–´ì§€ëŠ” í˜•íƒœ)
    """
    global failed_trajectories
    global reflection_map
    unique_trajectories = get_unique_trajectories(failed_trajectories)
    if len(unique_trajectories) > len(reflection_map) and len(unique_trajectories) < 4:
        print("generating reflections")
        reflection_map = task.generate_self_reflection(unique_trajectories, x)
    if prompt_sample == 'standard':
        prompt = task.standard_prompt_wrap(x, y)
    elif prompt_sample == 'cot':
        prompt = task.cot_prompt_wrap(x, y, reflection_map)
    else:
        raise ValueError(f'prompt_sample {prompt_sample} not recognized')
    # DEBUG: í”„ë¡¬í”„íŠ¸ ë˜í•‘ í™•ì¸
    # í™•ì¸í•  ê°’: x (ì…ë ¥ ì§ˆë¬¸), y (í˜„ì¬ trajectory), prompt (ìµœì¢… ë˜í•‘ëœ í”„ë¡¬í”„íŠ¸), prompt_sample
    # import pdb; pdb.set_trace()
    logging.info(f"PROMPT: {prompt}")
    # max_tokensë¥¼ ì¶©ë¶„íˆ í¬ê²Œ ì„¤ì •í•˜ì—¬ Thoughtì™€ Actionì´ ì™„ì „íˆ ìƒì„±ë˜ë„ë¡ í•¨
    # Thought + Actionì„ ìƒì„±í•˜ë ¤ë©´ ìµœì†Œ 1000-2000 í† í°ì´ í•„ìš”í•˜ë¯€ë¡œ 2000ìœ¼ë¡œ ì„¤ì •
    # stop í† í°("Observation")ì´ ë‚˜íƒ€ë‚˜ë©´ ìë™ìœ¼ë¡œ ì¤‘ë‹¨ë˜ë¯€ë¡œ ì•ˆì „í•¨
    samples = gpt(prompt, n=n_generate_sample, stop=stop, max_tokens=2000)
    return [y + _ for _ in samples]

def get_unique_trajectories(failed_trajectories, num=5):
    """
    ì‹¤íŒ¨í•œ ê¶¤ì ë“¤ ì¤‘ì—ì„œ ê³ ìœ í•œ ê²ƒë“¤ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    ë™ì¼í•œ ìµœì¢… ë‹µë³€ì„ ê°€ì§„ ê¶¤ì ë“¤ì€ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼í•˜ì—¬ ì œê±°í•©ë‹ˆë‹¤.
    ë°˜ì„±(reflection) ìƒì„± ì‹œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ê³ ìœ í•œ ì‹¤íŒ¨ ì‚¬ë¡€ë§Œ ì„ íƒí•©ë‹ˆë‹¤.
    
    Args:
        failed_trajectories: ì‹¤íŒ¨í•œ ê¶¤ì ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ (ê°ê°ì€ 'trajectory'ì™€ 'final_answer' í‚¤ë¥¼ ê°€ì§)
        num: ë°˜í™˜í•  ìµœëŒ€ ê³ ìœ  ê¶¤ì  ìˆ˜ (ê¸°ë³¸ê°’: 5)
    
    Returns:
        list: ê³ ìœ í•œ ê¶¤ì ë“¤ì˜ í…ìŠ¤íŠ¸ í‘œí˜„ ë¦¬ìŠ¤íŠ¸
    """
    unique_trajectories = []
    seen_final_answers = set()
    for traj in failed_trajectories:
        final_answer = traj.get('final_answer')
        if final_answer not in seen_final_answers:
            unique_trajectories.append(node_trajectory_to_text(traj['trajectory']))
            seen_final_answers.add(final_answer)
        if len(unique_trajectories) >= num:
            break
    return unique_trajectories

class Node:
    """
    íƒìƒ‰ íŠ¸ë¦¬ì˜ ë…¸ë“œë¥¼ ë‚˜íƒ€ë‚´ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    
    ê° ë…¸ë“œëŠ” í•˜ë‚˜ì˜ ìƒíƒœ(state)ë¥¼ ë‚˜íƒ€ë‚´ë©°, Thought-Action-Observation ì‹œí€€ìŠ¤ì˜ í•œ ë‹¨ê³„ë¥¼ í‘œí˜„í•©ë‹ˆë‹¤.
    MCTS(Monte Carlo Tree Search) ì•Œê³ ë¦¬ì¦˜ì—ì„œ ì‚¬ìš©ë˜ë©°, UCT(Upper Confidence Bound for Trees) ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    def __init__(self, state, question, parent=None):
        """
        ë…¸ë“œë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        
        Args:
            state: ë…¸ë“œì˜ ìƒíƒœ ë”•ì…”ë„ˆë¦¬ {'thought': str, 'action': str, 'observation': str}
            question: ì›ë³¸ ì§ˆë¬¸ ë˜ëŠ” í”„ë¡¬í”„íŠ¸
            parent: ë¶€ëª¨ ë…¸ë“œ (Noneì´ë©´ ë£¨íŠ¸ ë…¸ë“œ)
        """
        self.state = {'thought': '', 'action': '', 'observation': ''} if state is None else state
        self.parent = parent
        self.question = question
        self.children = []  # ìì‹ ë…¸ë“œë“¤ì˜ ë¦¬ìŠ¤íŠ¸
        self.visits = 0  # ì´ ë…¸ë“œê°€ ë°©ë¬¸ëœ íšŸìˆ˜
        self.value = 0  # ë…¸ë“œì˜ í‰ê·  ê°€ì¹˜
        self.depth = 0 if parent is None else parent.depth + 1  # íŠ¸ë¦¬ì—ì„œì˜ ê¹Šì´
        self.is_terminal = False  # ì¢…ë£Œ ë…¸ë“œ ì—¬ë¶€ (ë‹µì„ ì°¾ì•˜ê±°ë‚˜ ì‹¤íŒ¨)
        self.reward = 0  # ë³´ìƒ ê°’ (0 ë˜ëŠ” 1)
        self.exhausted = False  # ëª¨ë“  ìì‹ì´ ì¢…ë£Œ ë…¸ë“œì¸ì§€ ì—¬ë¶€
        self.em = 0  # Exact Match, í‰ê°€ ë©”íŠ¸ë¦­ (ì •í™•íˆ ì¼ì¹˜í•˜ëŠ”ì§€ ì—¬ë¶€)

    def uct(self):
        """
        UCT(Upper Confidence Bound for Trees) ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        
        íƒìƒ‰ê³¼ í™œìš©(exploration vs exploitation)ì˜ ê· í˜•ì„ ë§ì¶”ê¸° ìœ„í•œ ê°’ì…ë‹ˆë‹¤.
        ë†’ì€ UCT ê°’ì„ ê°€ì§„ ë…¸ë“œê°€ ìš°ì„ ì ìœ¼ë¡œ ì„ íƒë©ë‹ˆë‹¤.
        
        Returns:
            float: UCT ê°’ (visitsê°€ 0ì´ë©´ valueë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜)
        """
        if self.visits == 0:
            return self.value
        return self.value / self.visits + np.sqrt(2 * np.log(self.parent.visits) / self.visits)
    
    def __str__(self):
        """
        ë…¸ë“œì˜ ë¬¸ìì—´ í‘œí˜„ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        ë””ë²„ê¹…ì´ë‚˜ ë¡œê¹…ì— ì‚¬ìš©ë˜ëŠ” ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ í˜•ì‹ì…ë‹ˆë‹¤.
        
        Returns:
            str: ë…¸ë“œì˜ ì£¼ìš” ì •ë³´ë¥¼ í¬í•¨í•œ ë¬¸ìì—´
        """
        return f"Node(depth={self.depth}, value={self.value:.2f}, visits={self.visits}, thought={self.state['thought']}, action={self.state['action']}, observation={self.state['observation']})"
    
    def to_dict(self):
        """
        ë…¸ë“œë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        ì§ë ¬í™”ë‚˜ ì €ì¥ ëª©ì ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì¬ê·€ì ìœ¼ë¡œ ìì‹ ë…¸ë“œë“¤ë„ ë³€í™˜í•©ë‹ˆë‹¤.
        
        Returns:
            dict: ë…¸ë“œì˜ ëª¨ë“  ì •ë³´ë¥¼ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
        """
        return {
            'state': self.state,
            'question': self.question,
            'parent': self.parent.to_dict() if self.parent else None,
            'children': [child.to_dict() for child in self.children],
            'visits': self.visits,
            'value': self.value,
            'depth': self.depth,
            'is_terminal': self.is_terminal,
            'reward': self.reward,
            'em': self.em,
        }
    
def node_trajectory_to_text(node_string):
    """
    ë…¸ë“œì˜ ë¬¸ìì—´ í‘œí˜„ì„ ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    ë…¸ë“œì˜ __str__ ë©”ì„œë“œë¡œ ìƒì„±ëœ ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬
    "Thought N: ...", "Action N: ...", "Observation N: ..." í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    Args:
        node_string: ë…¸ë“œì˜ ë¬¸ìì—´ í‘œí˜„ (ì˜ˆ: "Node(depth=1, thought=..., action=..., observation=...)")
    
    Returns:
        str: í¬ë§·íŒ…ëœ ê¶¤ì  í…ìŠ¤íŠ¸
    """
    lines = node_string.split('\n')
    formatted_lines = []
    for line in lines:
        try:
            depth = int(line.split(",")[0].split("=")[1].strip())
            thought = line.split(", thought=")[1].split(", action=")[0].strip()
            action = line.split(", action=")[1].split(", observation=")[0].strip()
            observation = line.split(", observation=")[1].split(")")[0].strip()
        except IndexError:
            continue
        
        if depth != 0:
            if thought:
                formatted_lines.append(f"Thought {depth}: {thought}")
            if action:
                formatted_lines.append(f"Action {depth}: {action}")
            if observation:
                formatted_lines.append(f"Observation {depth}: {observation}")
    
    return '\n'.join(formatted_lines)

def collect_all_nodes(node):
    """
    ì£¼ì–´ì§„ ë…¸ë“œë¶€í„° ì‹œì‘í•˜ì—¬ ëª¨ë“  í•˜ìœ„ ë…¸ë“œë“¤ì„ ì¬ê·€ì ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    
    íŠ¸ë¦¬ì˜ íŠ¹ì • ë…¸ë“œë¥¼ ë£¨íŠ¸ë¡œ í•˜ëŠ” ì„œë¸ŒíŠ¸ë¦¬ì˜ ëª¨ë“  ë…¸ë“œë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    ë””ë²„ê¹…ì´ë‚˜ í†µê³„ ìˆ˜ì§‘ì— ì‚¬ìš©ë©ë‹ˆë‹¤.
    
    Args:
        node: ìˆ˜ì§‘ì„ ì‹œì‘í•  ë…¸ë“œ
    
    Returns:
        list: ë…¸ë“œì™€ ê·¸ ëª¨ë“  ìì‹ ë…¸ë“œë“¤ì˜ ë¦¬ìŠ¤íŠ¸
    """
    nodes = [node]
    for child in node.children:
        nodes.extend(collect_all_nodes(child))
    return nodes

def collect_trajectory(node):
    """
    ë…¸ë“œì—ì„œ ë£¨íŠ¸ê¹Œì§€ì˜ ì „ì²´ ê¶¤ì ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    
    íŠ¹ì • ë…¸ë“œì—ì„œ ì‹œì‘í•˜ì—¬ ë¶€ëª¨ ë…¸ë“œë¥¼ ë”°ë¼ ì˜¬ë¼ê°€ë©° ë£¨íŠ¸ê¹Œì§€ì˜ ê²½ë¡œë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    ì‹¤íŒ¨í•œ ê¶¤ì ì„ ê¸°ë¡í•˜ê±°ë‚˜ ë””ë²„ê¹…ì— ì‚¬ìš©ë©ë‹ˆë‹¤.
    
    Args:
        node: ê¶¤ì ì„ ìˆ˜ì§‘í•  ì‹œì‘ ë…¸ë“œ
    
    Returns:
        str: ë£¨íŠ¸ë¶€í„° í•´ë‹¹ ë…¸ë“œê¹Œì§€ì˜ ê¶¤ì ì„ ë‚˜íƒ€ë‚´ëŠ” ë¬¸ìì—´
    """
    trajectory = []
    while node:
        trajectory.append(str(node))
        node = node.parent
    return '\n'.join(reversed(trajectory))

def lats_search(args, task, idx, iterations=30, to_print=True):
    """
    LATS (Language Agent Tree Search) ì•Œê³ ë¦¬ì¦˜ì˜ ë©”ì¸ ê²€ìƒ‰ í•¨ìˆ˜ì…ë‹ˆë‹¤.
    
    MCTS ê¸°ë°˜ì˜ íŠ¸ë¦¬ íƒìƒ‰ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ ì°¾ìŠµë‹ˆë‹¤.
    ê° ë°˜ë³µì—ì„œ Selection, Expansion, Simulation, Backpropagation ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    ì•Œê³ ë¦¬ì¦˜ íë¦„:
    1. ë£¨íŠ¸ ë…¸ë“œ ìƒì„± (ì´ˆê¸° ì§ˆë¬¸)
    2. ë°˜ë³µ (iterations íšŸìˆ˜ë§Œí¼):
       - Selection: UCTë¥¼ ì‚¬ìš©í•˜ì—¬ íƒìƒ‰í•  ë…¸ë“œ ì„ íƒ
       - Expansion: ì„ íƒëœ ë…¸ë“œë¥¼ í™•ì¥í•˜ì—¬ ìì‹ ë…¸ë“œ ìƒì„±
       - Simulation: ì‹œë®¬ë ˆì´ì…˜ì„ í†µí•´ ë…¸ë“œ ê°€ì¹˜ ì¶”ì •
       - Backpropagation: ê²°ê³¼ë¥¼ ë¶€ëª¨ ë…¸ë“œë“¤ë¡œ ì—­ì „íŒŒ
    3. ìµœì¢…ì ìœ¼ë¡œ ê°€ì¥ ì¢‹ì€ ë…¸ë“œ ë°˜í™˜
    
    Args:
        args: ëª…ë ¹ì¤„ ì¸ì ê°ì²´ (backend, temperature, n_generate_sample ë“± í¬í•¨)
        task: Task ê°ì²´ (í”„ë¡¬í”„íŠ¸ ìƒì„±, ê°€ì¹˜ í‰ê°€ ë©”ì„œë“œ í¬í•¨)
        idx: ë°ì´í„°ì…‹ì—ì„œì˜ ì¸ë±ìŠ¤
        iterations: ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ (ê¸°ë³¸ê°’: 30)
        to_print: ê²°ê³¼ë¥¼ ì¶œë ¥í• ì§€ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
    
    Returns:
        tuple: (ìµœì¢… ìƒíƒœ, ê°€ì¹˜, ëª¨ë“  ë…¸ë“œ, ë³´ìƒ, ì •í™•ë„)
            - ìµœì¢… ìƒíƒœ: ì°¾ì€ ë‹µì˜ ìƒíƒœ
            - ê°€ì¹˜: ìµœì¢… ë…¸ë“œì˜ ê°€ì¹˜
            - ëª¨ë“  ë…¸ë“œ: íƒìƒ‰ ì¤‘ ìƒì„±ëœ ëª¨ë“  ë…¸ë“œ
            - ë³´ìƒ: ìµœì¢… ë³´ìƒ (0 ë˜ëŠ” 1)
            - ì •í™•ë„: Exact Match ì ìˆ˜
    """
    global gpt
    global failed_trajectories
    global reflection_map
    local_env = _ensure_env(args)
    gpt = partial(gpt, model=args.backend, temperature=args.temperature, local_model_name=getattr(args, "local_model_name", None))
    
    # ë°ì´í„°ì…‹ íƒ€ì…ì— ë”°ë¼ ì…ë ¥ ì²˜ë¦¬ ë°©ì‹ ê²°ì •
    dataset_type = getattr(args, "dataset_type", None) or os.getenv("DATASET_TYPE", "hotpotqa")
    
    if dataset_type == "perturbqa":
        # PerturbQAì˜ ê²½ìš°: task.get_input()ìœ¼ë¡œ _build_full_prompt()ì—ì„œ ìƒì„±í•œ ì „ì²´ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        # ì‹œìŠ¤í…œ/ì‚¬ìš©ì/ì–´ì‹œìŠ¤í„´íŠ¸ íƒœê·¸ì™€ ì»¨í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ì™„ì „í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì–»ìŒ
        x = task.get_input(idx)
        # ì•¡ì…˜ ì‹¤í–‰ì„ ìœ„í•´ í™˜ê²½ ì´ˆê¸°í™” í•„ìš”
        local_env.reset(idx=idx)
    else:
        # HotPotQAì˜ ê²½ìš°: env.reset()ì´ "Question: {question}" í˜•ì‹ ë°˜í™˜
        # env.reset()ì´ ì´ë¯¸ í™˜ê²½ì„ ì´ˆê¸°í™”í•˜ë¯€ë¡œ ë°˜í™˜ê°’ ì‚¬ìš©
        x = local_env.reset(idx=idx)
    
    # import pdb; pdb.set_trace()
    if to_print:
        print(idx, x)
    root = Node(state=None, question=x)  # ë£¨íŠ¸ ë…¸ë“œ ìƒì„±
    all_nodes = []  # ëª¨ë“  ë…¸ë“œë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    failed_trajectories = []  # ì‹¤íŒ¨í•œ ê¶¤ì ë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    terminal_nodes = []  # ì¢…ë£Œ ë…¸ë“œë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    reflection_map = []  # ìê°€ ë°˜ì„± ë§µ (ì‹¤íŒ¨í•œ ê¶¤ì ì— ëŒ€í•œ ë°˜ì„±)
    logging.basicConfig(filename=args.log, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', filemode='a')
    
    # ì´ˆê¸° ìƒíƒœ ë¡œê¹…
    logging.info("=" * 80)
    logging.info(f"LATS SEARCH STARTED - Question Index: {idx}")
    logging.info("=" * 80)
    logging.info(f"Question Preview: {x[:200]}..." if len(x) > 200 else f"Question: {x}")
    logging.info(f"Total Iterations: {iterations}")
    logging.info(f"Root Node Created: {format_node_detail(root)}")
    
    for i in range(iterations):
        logging.info("")
        logging.info("=" * 80)
        logging.info(f"ITERATION {i + 1}/{iterations}")
        logging.info("=" * 80)
        
        # Selection ë‹¨ê³„: UCTë¥¼ ì‚¬ìš©í•˜ì—¬ íƒìƒ‰í•  ë…¸ë“œ ì„ íƒ
        logging.info("â”€" * 80)
        logging.info("STEP 1: SELECTION")
        logging.info("â”€" * 80)
        node = select_node(root)

        # ì¢…ë£Œ ë…¸ë“œì´ê±°ë‚˜ ë³´ìƒì´ 0ì¸ ê²½ìš° ë‹¤ì‹œ ì„ íƒ (ë°±íŠ¸ë˜í‚¹)
        while node is None or (node.is_terminal and node.reward != 1):
            if node is None:
                logging.warning(f"  âš ï¸  Selected node is None, reselecting...")
            elif node.is_terminal and node.reward != 1:
                logging.warning(f"  âš ï¸  Terminal node with reward 0 found at depth {node.depth}, reselecting...")
                logging.info(f"  Node details: {format_node_detail(node)}")
            node = select_node(root)
        
        # ëª¨ë“  ê²½ë¡œê°€ ê³ ê°ˆëœ ê²½ìš° ê²€ìƒ‰ ì¢…ë£Œ
        if node is None:
            logging.warning("=" * 80)
            logging.warning("SEARCH TERMINATED: All paths lead to terminal nodes with reward 0")
            logging.warning("=" * 80)
            log_tree_statistics(root)
            break

        # ì„ íƒëœ ë…¸ë“œ ìƒì„¸ ì •ë³´ ë¡œê¹…
        logging.info(f"  âœ“ Selected Node:")
        logging.info(format_node_detail(node))
        
        # ì„±ê³µí•œ ì¢…ë£Œ ë…¸ë“œë¥¼ ì°¾ì€ ê²½ìš° ì¦‰ì‹œ ë°˜í™˜
        if node.is_terminal and node.reward == 1:
            logging.info("=" * 80)
            logging.info(f"ğŸ‰ SUCCESS! Terminal node with reward 1 found at iteration {i + 1}")
            logging.info("=" * 80)
            logging.info(f"Final Node: {format_node_detail(node)}")
            log_tree_statistics(root)
            return node.state, node.value, all_nodes, node.reward, node.em
        
        # Expansion ë‹¨ê³„: ì„ íƒëœ ë…¸ë“œë¥¼ í™•ì¥í•˜ì—¬ ìì‹ ë…¸ë“œ ìƒì„±
        logging.info("â”€" * 80)
        logging.info("STEP 2: EXPANSION")
        logging.info("â”€" * 80)
        logging.info(f"  Expanding node at depth {node.depth}...")
        expand_node(node, args, task)
        logging.info(f"  âœ“ Expanded: {len(node.children)} new children created")

        # ê¹Šì´ ì œí•œì— ë„ë‹¬í–ˆê±°ë‚˜ ìì‹ì´ ì—†ëŠ” ê²½ìš° ë‹¤ì‹œ ì„ íƒ
        while node.is_terminal or not node.children:
            if node.is_terminal:
                logging.warning(f"  âš ï¸  Node is terminal, reselecting...")
            elif not node.children:
                logging.warning(f"  âš ï¸  Node has no children, reselecting...")
            node = select_node(root)
            expand_node(node, args, task)

        # ìì‹ ë…¸ë“œë“¤ ì •ë³´ ë¡œê¹…
        logging.info(f"  Children created:")
        for j, child in enumerate(node.children):
            logging.info(f"    Child {j+1}: Depth={child.depth}, Value={child.value:.4f}, Terminal={child.is_terminal}, Reward={child.reward}")
            if child.state.get('thought'):
                thought_short = child.state['thought'][:80] + '...' if len(child.state['thought']) > 80 else child.state['thought']
                logging.info(f"      Thought: {thought_short}")

        # Evaluation ë‹¨ê³„: ìì‹ ë…¸ë“œë“¤ì˜ ê°€ì¹˜ í‰ê°€
        logging.info("â”€" * 80)
        logging.info("STEP 3: EVALUATION")
        logging.info("â”€" * 80)
        logging.info(f"  Evaluating {len(node.children)} children...")
        value = evaluate_node(node, args, task)
        logging.info(f"  âœ“ Evaluation complete. Average value: {value:.4f}")
        
        # Simulation ë‹¨ê³„: ê°€ì¥ ë†’ì€ ê°€ì¹˜ë¥¼ ê°€ì§„ ìì‹ ë…¸ë“œì—ì„œ ì‹œë®¬ë ˆì´ì…˜ ìˆ˜í–‰
        best_child = max(node.children, key=lambda child: child.value)
        logging.info("â”€" * 80)
        logging.info("STEP 4: SIMULATION (ROLLOUT)")
        logging.info("â”€" * 80)
        logging.info(f"  Starting rollout from best child (value={best_child.value:.4f}):")
        logging.info(format_node_detail(best_child))
        reward, terminal_node = rollout(best_child, args, task, idx, max_depth=4)

        terminal_nodes.append(terminal_node)
        
        logging.info(f"  âœ“ Rollout complete. Reward: {reward}, Terminal depth: {terminal_node.depth}")
        logging.info(f"  Terminal node: {format_node_detail(terminal_node)}")

        # ì‹œë®¬ë ˆì´ì…˜ ì¤‘ ì„±ê³µí•œ ê²½ë¡œë¥¼ ì°¾ì€ ê²½ìš° ì¦‰ì‹œ ë°˜í™˜
        if terminal_node.reward == 1:
            logging.info("=" * 80)
            logging.info("ğŸ‰ SUCCESS! Successful trajectory found during simulation")
            logging.info("=" * 80)
            log_tree_statistics(root)
            return terminal_node.state, terminal_node.value, [], terminal_node.reward, terminal_node.em

        # Backpropagation ë‹¨ê³„: ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ë¥¼ ë¶€ëª¨ ë…¸ë“œë“¤ë¡œ ì—­ì „íŒŒ
        logging.info("â”€" * 80)
        logging.info("STEP 5: BACKPROPAGATION")
        logging.info("â”€" * 80)
        logging.info(f"  Backpropagating reward {reward} from depth {terminal_node.depth}...")
        backpropagate(terminal_node, reward)
        all_nodes = [(node, node.value) for node in collect_all_nodes(root)]

        # íŠ¸ë¦¬ ì „ì²´ì—ì„œ ë³´ìƒì´ 1ì¸ ì¢…ë£Œ ë…¸ë“œ í™•ì¸ (ì„±ê³µí•œ ê²½ë¡œê°€ ìˆëŠ”ì§€ ì²´í¬)
        terminal_nodes_with_reward_1 = [node for node in collect_all_nodes(root) if node.is_terminal and node.reward == 1]
        if terminal_nodes_with_reward_1:
            logging.info("=" * 80)
            logging.info(f"ğŸ‰ SUCCESS! Terminal node with reward 1 found at iteration {i + 1}")
            logging.info("=" * 80)
            best_node = max(terminal_nodes_with_reward_1, key=lambda x: x.value)
            logging.info(f"Best node: {format_node_detail(best_node)}")
            log_tree_statistics(root)
            return best_node.state, best_node.value, all_nodes, best_node.reward, best_node.em
    
        # ë°˜ë³µ ì¢…ë£Œ ì‹œ íŠ¸ë¦¬ ìƒíƒœ ìš”ì•½
        logging.info("â”€" * 80)
        logging.info(f"Iteration {i + 1} Summary:")
        logging.info(f"  Total nodes in tree: {len(all_nodes)}")
        logging.info(f"  Terminal nodes: {len(terminal_nodes)}")
        logging.info(f"  Failed trajectories: {len(failed_trajectories)}")
        
        # ì£¼ê¸°ì ìœ¼ë¡œ íŠ¸ë¦¬ êµ¬ì¡° ì¶œë ¥ (ë§¤ 5ë²ˆì§¸ ë°˜ë³µë§ˆë‹¤)
        if (i + 1) % 5 == 0:
            log_tree_structure(root, max_depth=3)
            log_tree_statistics(root)
    
    # ëª¨ë“  ë°˜ë³µì´ ëë‚œ í›„ ìµœì¢… ê²°ê³¼ ì„ íƒ
    # import pdb; pdb.set_trace()
    logging.info("")
    logging.info("=" * 80)
    logging.info("FINAL RESULT SELECTION")
    logging.info("=" * 80)
    
    all_nodes_list = collect_all_nodes(root)
    all_nodes_list.extend(terminal_nodes)
    
    # ìµœì¢… íŠ¸ë¦¬ êµ¬ì¡° ë° í†µê³„ ì¶œë ¥
    log_tree_structure(root, max_depth=5)
    log_tree_statistics(root)
    
    # ë³´ìƒì´ ê°€ì¥ ë†’ì€ ë…¸ë“œ ì„ íƒ (ë³´ìƒì´ 1ì¸ ë…¸ë“œê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„, ì—†ìœ¼ë©´ ê°€ì¥ ë†’ì€ ë³´ìƒ)
    best_child = max(all_nodes_list, key=lambda x: x.reward)
    failed_trajectories = []
    
    logging.info("â”€" * 80)
    if best_child.reward == 1:
        logging.info("âœ… FINAL RESULT: Successful trajectory found")
    else:
        logging.warning("âŒ FINAL RESULT: Unsuccessful trajectory found")
    logging.info("â”€" * 80)
    logging.info(f"Best Node Selected:")
    logging.info(format_node_detail(best_child))
    
    if best_child is None:
        best_child = root
        logging.warning("  âš ï¸  Best child was None, using root node")
    
    logging.info("=" * 80)
    logging.info(f"LATS SEARCH COMPLETED - Question Index: {idx}")
    logging.info("=" * 80)
    logging.info("")
    
    return best_child.state, best_child.value, all_nodes, best_child.reward, best_child.em

def select_node(node):
    """
    UCT ê°’ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒìœ¼ë¡œ íƒìƒ‰í•  ë…¸ë“œë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
    
    MCTS ì•Œê³ ë¦¬ì¦˜ì˜ Selection ë‹¨ê³„ì…ë‹ˆë‹¤. íƒìƒ‰ê³¼ í™œìš©ì˜ ê· í˜•ì„ ë§ì¶”ê¸° ìœ„í•´
    UCT ê°’ì´ ê°€ì¥ ë†’ì€ ë…¸ë“œë¥¼ ì„ íƒí•©ë‹ˆë‹¤. ì¢…ë£Œ ë…¸ë“œë‚˜ ëª¨ë“  ìì‹ì´ ì¢…ë£Œì¸ ê²½ìš°
    ë°±íŠ¸ë˜í‚¹ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        node: ì„ íƒì„ ì‹œì‘í•  ë…¸ë“œ (ë³´í†µ ë£¨íŠ¸ ë…¸ë“œ)
    
    Returns:
        Node: ì„ íƒëœ ë…¸ë“œ (ëª¨ë“  ê²½ë¡œê°€ ê³ ê°ˆë˜ë©´ None)
    """
    # import pdb; pdb.set_trace()
    while node and node.children:
        logging.info(f"  Selecting from {len(node.children)} children at depth {node.depth}")
        
        # ìì‹ ë…¸ë“œë“¤ì˜ UCT ê°’ ë¡œê¹…
        for j, child in enumerate(node.children):
            uct_val = child.uct() if child.parent else 0.0
            status = "TERMINAL" if child.is_terminal else "ACTIVE"
            reward_info = f"R:{child.reward}" if child.is_terminal else ""
            logging.info(f"    Child {j+1}: {status} | UCT:{uct_val:.4f} | Value:{child.value:.4f} | Visits:{child.visits} {reward_info}")
        
        terminal_children = [child for child in node.children if child.is_terminal]
        terminal_status = [child.is_terminal for child in node.children]
        
        # ëª¨ë“  ìì‹ì´ ì¢…ë£Œ ë…¸ë“œì¸ ê²½ìš° ë°±íŠ¸ë˜í‚¹
        if len(terminal_children) == len(node.children):
            logging.warning(f"  âš ï¸  All children are terminal at depth {node.depth}. Backtracking...")
            if node.parent:  
                node.parent.children.remove(node)
            node = node.parent  
            continue  
        
        # ë³´ìƒì´ 1ì¸ ì¢…ë£Œ ë…¸ë“œê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ë°˜í™˜ (ì„±ê³µí•œ ê²½ë¡œ ë°œê²¬)
        node_with_reward_1 = next((child for child in terminal_children if child.reward == 1), None)
        if node_with_reward_1:
            logging.info(f"  âœ“ Found terminal node with reward 1 at depth {node.depth}")
            return node_with_reward_1
        
        # UCT ê°’ì´ ê°€ì¥ ë†’ì€ ë¹„ì¢…ë£Œ ìì‹ ë…¸ë“œ ì„ íƒ
        non_terminal_children = [child for child in node.children if not child.is_terminal]
        if non_terminal_children:
            node = max(non_terminal_children, key=lambda child: child.uct())
        else:
            node = None

        # ì„ íƒëœ ë…¸ë“œê°€ ì¢…ë£Œ ë…¸ë“œì´ê³  ë³´ìƒì´ 1ì´ ì•„ë‹Œ ê²½ìš°, ë‹¤ì‹œ ì„ íƒ
        while node and node.is_terminal and node.reward != 1:
            non_terminal_children = [child for child in node.parent.children if not child.is_terminal]
            node = max(non_terminal_children, key=lambda child: child.uct(), default=None) if non_terminal_children else None
            
        logging.info(f"  âœ“ Selected node at depth {node.depth} with UCT {node.uct():.4f}")
        logging.info(format_node_detail(node))
        
    return node  # ëª¨ë“  ê²½ë¡œê°€ ê³ ê°ˆë˜ë©´ None ë°˜í™˜

def expand_node(node, args, task):
    """
    ë…¸ë“œë¥¼ í™•ì¥í•˜ì—¬ ìì‹ ë…¸ë“œë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    MCTS ì•Œê³ ë¦¬ì¦˜ì˜ Expansion ë‹¨ê³„ì…ë‹ˆë‹¤. ì„ íƒëœ ë…¸ë“œì—ì„œ LLMì„ ì‚¬ìš©í•˜ì—¬
    ë‹¤ìŒ ê°€ëŠ¥í•œ ì•¡ì…˜ë“¤ì„ ìƒì„±í•˜ê³ , ê° ì•¡ì…˜ì— ëŒ€í•´ ìƒˆë¡œìš´ ë…¸ë“œë¥¼ ë§Œë“­ë‹ˆë‹¤.
    ìµœëŒ€ ê¹Šì´(7)ì— ë„ë‹¬í•˜ë©´ ë…¸ë“œë¥¼ ì¢…ë£Œ ë…¸ë“œë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
    
    Args:
        node: í™•ì¥í•  ë…¸ë“œ
        args: ëª…ë ¹ì¤„ ì¸ì ê°ì²´ (n_generate_sample ë“± í¬í•¨)
        task: Task ê°ì²´
    """
    # import pdb; pdb.set_trace()
    if node.depth >= 7:
        logging.info("Depth limit reached")
        print("Depth limit reached")
        node.is_terminal = True
        return
    new_nodes = generate_new_states(node, args, task, args.n_generate_sample)
    node.children.extend(new_nodes)

def rollout(node, args, task, idx, max_depth=4):
    """
    ì‹œë®¬ë ˆì´ì…˜ì„ í†µí•´ ë…¸ë“œì˜ ê°€ì¹˜ë¥¼ ì¶”ì •í•©ë‹ˆë‹¤.
    
    MCTS ì•Œê³ ë¦¬ì¦˜ì˜ Simulation ë‹¨ê³„ì…ë‹ˆë‹¤. ì„ íƒëœ ë…¸ë“œì—ì„œ ì‹œì‘í•˜ì—¬
    íƒìš•ì (greedy) ë°©ì‹ìœ¼ë¡œ ìµœê³  ê°€ì¹˜ì˜ ìì‹ì„ ì„ íƒí•˜ë©° ì§„í–‰í•©ë‹ˆë‹¤.
    ì¢…ë£Œ ë…¸ë“œì— ë„ë‹¬í•˜ê±°ë‚˜ ìµœëŒ€ ê¹Šì´ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ì‹œë®¬ë ˆì´ì…˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        node: ì‹œë®¬ë ˆì´ì…˜ì„ ì‹œì‘í•  ë…¸ë“œ
        args: ëª…ë ¹ì¤„ ì¸ì ê°ì²´
        task: Task ê°ì²´
        idx: ë°ì´í„° ì¸ë±ìŠ¤
        max_depth: ì‹œë®¬ë ˆì´ì…˜ì˜ ìµœëŒ€ ê¹Šì´ (ê¸°ë³¸ê°’: 4)
    
    Returns:
        tuple: (í‰ê·  ë³´ìƒ, ì¢…ë£Œ ë…¸ë“œ)
            - í‰ê·  ë³´ìƒ: ì‹œë®¬ë ˆì´ì…˜ ì¤‘ ì–»ì€ ë³´ìƒë“¤ì˜ í‰ê· 
            - ì¢…ë£Œ ë…¸ë“œ: ì‹œë®¬ë ˆì´ì…˜ì´ ì¢…ë£Œëœ ë…¸ë“œ
    """
    # import pdb; pdb.set_trace()
    logging.info("ROLLING OUT")
    depth = node.depth
    n = 5
    rewards = [0]
    while not node.is_terminal and depth < max_depth:
        # Generate new states
        logging.info(f"ROLLING OUT {depth}")
        new_states = []
        values = []
        while len(new_states) == 0:
            new_states = generate_new_states(node, args, task, n)

        for state in new_states:
            if state.is_terminal:
                return state.reward, state
                
        child_prompts = [generate_prompt(child) for child in new_states if not child.is_terminal and child is not None]
        #new_state = new_state[0]
        while len(values) == 0:
            values = get_values(task, node.question, child_prompts, args.n_evaluate_sample)
        max_value_index = values.index(max(values))
        rewards.append(max(values))
        node = new_states[max_value_index] 
        depth += 1
        if depth == max_depth:
            rewards = [-1]
    
    avg_reward = sum(rewards) / len(rewards) if rewards else 0
    logging.info(f"  âœ“ Rollout finished. Depth reached: {depth}, Average reward: {avg_reward:.4f}")
    logging.info(f"  Final node: {format_node_detail(node)}")
    return avg_reward, node

def generate_new_states(node, args, task, n):
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ í˜„ì¬ ë…¸ë“œì—ì„œ ê°€ëŠ¥í•œ ìƒˆë¡œìš´ ìƒíƒœë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    ë…¸ë“œì˜ í˜„ì¬ ìƒíƒœë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜í•˜ê³ , LLMì„ í˜¸ì¶œí•˜ì—¬ ë‹¤ìŒ Thoughtì™€ Actionì„ ìƒì„±í•©ë‹ˆë‹¤.
    ê° ìƒì„±ëœ ì•¡ì…˜ì„ í™˜ê²½ì—ì„œ ì‹¤í–‰í•˜ì—¬ Observationì„ ì–»ê³ , ìƒˆë¡œìš´ ë…¸ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    ì¤‘ë³µëœ ìƒíƒœëŠ” ì œê±°í•˜ì—¬ ê³ ìœ í•œ ë…¸ë“œë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        node: ìƒˆë¡œìš´ ìƒíƒœë¥¼ ìƒì„±í•  ê¸°ì¤€ ë…¸ë“œ
        args: ëª…ë ¹ì¤„ ì¸ì ê°ì²´ (prompt_sample ë“± í¬í•¨)
        task: Task ê°ì²´
        n: ìƒì„±í•  ì•¡ì…˜ í›„ë³´ ìˆ˜
    
    Returns:
        list: ìƒì„±ëœ ìƒˆë¡œìš´ ë…¸ë“œë“¤ì˜ ë¦¬ìŠ¤íŠ¸ (ì¤‘ë³µ ì œê±°ë¨)
    """
    global failed_trajectories
    # import pdb; pdb.set_trace()
    prompt = generate_prompt(node)
    sampled_actions = get_samples(task, prompt, f"Thought {node.depth + 1}: ", n, prompt_sample=args.prompt_sample, stop="Observation")
    logging.info(f"SAMPLED ACTION: {sampled_actions}")
    tried_actions = []
    
    unique_states = {}  # ê³ ìœ í•œ ìƒíƒœë¥¼ ì €ì¥í•˜ê¸° ìœ„í•œ ë”•ì…”ë„ˆë¦¬
    for action in sampled_actions:
        new_state = node.state.copy()  # ë¶€ëª¨ ë…¸ë“œì˜ ìƒíƒœë¥¼ ë³µì‚¬

        # ìƒì„±ëœ ì•¡ì…˜ì—ì„œ Thoughtì™€ Action ë¼ì¸ ì¶”ì¶œ
        thought_line = next((line.split(":")[1].strip() for line in action.split("\n") if line.startswith(f"Thought {node.depth + 1}")), '')
        action_line = next((line.split(":")[1].strip() for line in action.split("\n") if line.startswith("Action") and ":" in line), None)

        # Thoughtì™€ Actionì„ ì¡°í•©í•˜ì—¬ ê³ ìœ  í‚¤ ìƒì„±
        unique_key = f"{thought_line}::{action_line}"
        
        if unique_key in unique_states:
            continue  # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ìƒíƒœëŠ” ê±´ë„ˆëœ€

        tried_actions.append(action_line)
        
        if action_line:
            # ì•¡ì…˜ íƒ€ì…ê³¼ íŒŒë¼ë¯¸í„° ì¶”ì¶œ (ì˜ˆ: "Search[entity]" -> "Search", "entity")
            action_type = action_line.split('[')[0] if '[' in action_line else action_line
            action_param = action_line.split('[')[1].split(']')[0] if '[' in action_line else ""

            # í™˜ê²½ì—ì„œ ì•¡ì…˜ ì‹¤í–‰
            obs, r, done, info = step(env, f"{action_type.lower()}[{action_param}]")

            # ìƒˆë¡œìš´ ìƒíƒœ ë”•ì…”ë„ˆë¦¬ ì—…ë°ì´íŠ¸
            new_state['thought'] = thought_line
            new_state['action'] = action_line
            new_state['observation'] = obs

            # ìƒˆë¡œìš´ ë…¸ë“œ ìƒì„±
            new_node = Node(state=new_state, question=node.question, parent=node)
            new_node.is_terminal = r == 1 or done  # ë³´ìƒì´ 1ì´ê±°ë‚˜ ì—í”¼ì†Œë“œê°€ ëë‚˜ë©´ ì¢…ë£Œ
            new_node.reward = r
            new_node.depth = node.depth + 1
            if r == 1:
                new_node.em = info.get('em')  # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²½ìš° EM ì €ì¥
            unique_states[unique_key] = new_node  # ê³ ìœ  ìƒíƒœ ë”•ì…”ë„ˆë¦¬ì— ì¶”ê°€
            logging.info(f"  âœ“ New Node Created:")
            logging.info(format_node_detail(new_node))
            logging.info(f"  Environment Feedback: {info}")

            # ì‹¤íŒ¨í•œ ê¶¤ì  ê¸°ë¡ (ë³´ìƒì´ 0ì´ê³  ì¢…ë£Œëœ ê²½ìš°)
            if new_node.is_terminal and r == 0:
                trajectory = collect_trajectory(new_node)
                failed_trajectories.append({'trajectory': trajectory, 'final_answer': f"{action_type.lower()}[{action_param}]"})

    return list(unique_states.values())  # ê³ ìœ í•œ ë…¸ë“œë“¤ì˜ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜


def evaluate_node(node, args, task):
    """
    ë…¸ë“œì˜ ìì‹ ë…¸ë“œë“¤ì„ í‰ê°€í•˜ì—¬ ê°ê°ì— ê°€ì¹˜ë¥¼ í• ë‹¹í•©ë‹ˆë‹¤.
    
    MCTS ì•Œê³ ë¦¬ì¦˜ì˜ Evaluation ë‹¨ê³„ì…ë‹ˆë‹¤. ë…¸ë“œì˜ ëª¨ë“  ë¹„ì¢…ë£Œ ìì‹ ë…¸ë“œë“¤ì— ëŒ€í•´
    LLMì„ ì‚¬ìš©í•˜ì—¬ ê°€ì¹˜ë¥¼ í‰ê°€í•˜ê³ , ê° ìì‹ ë…¸ë“œì˜ value ì†ì„±ì— í• ë‹¹í•©ë‹ˆë‹¤.
    ë…¸ë“œì˜ ì „ì²´ ê°€ì¹˜ëŠ” ìì‹ë“¤ì˜ ê°€ì¹˜ í‰ê· ìœ¼ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤.
    
    Args:
        node: í‰ê°€í•  ë…¸ë“œ (ìì‹ ë…¸ë“œë“¤ì˜ ê°€ì¹˜ë¥¼ í‰ê°€)
        args: ëª…ë ¹ì¤„ ì¸ì ê°ì²´ (n_evaluate_sample ë“± í¬í•¨)
        task: Task ê°ì²´
    
    Returns:
        float: ìì‹ ë…¸ë“œë“¤ì˜ í‰ê·  ê°€ì¹˜
    """
    # # import pdb; pdb.set_trace()
    child_prompts = [generate_prompt(child) for child in node.children if not child.is_terminal]
    votes = get_values(task, node.question, child_prompts, args.n_evaluate_sample)
    
    logging.info(f"Length of votes: {len(votes)}")
    logging.info(f"Length of node.children: {len(node.children)}")
    
    # votes ë¦¬ìŠ¤íŠ¸ë¥¼ ë¯¸ë¦¬ í• ë‹¹ (ì¢…ë£Œ ë…¸ë“œì— ëŒ€í•´ì„œëŠ” 0ìœ¼ë¡œ ì±„ì›€)
    votes = votes + [0] * (len(node.children) - len(votes))
    for i, child in enumerate(node.children):
        child.value = votes[i]  # ê° ìì‹ ë…¸ë“œì— ê°€ì¹˜ í• ë‹¹
    
    return sum(votes) / len(votes) if votes else 0  # í‰ê·  ê°€ì¹˜ ë°˜í™˜


def format_node_detail(node):
    """
    ë…¸ë“œì˜ ìƒì„¸ ì •ë³´ë¥¼ í¬ë§·íŒ…í•©ë‹ˆë‹¤.
    
    Args:
        node: í¬ë§·íŒ…í•  ë…¸ë“œ
    
    Returns:
        str: ë…¸ë“œì˜ ìƒì„¸ ì •ë³´ ë¬¸ìì—´
    """
    if node is None:
        return "None"
    
    thought_preview = (node.state.get('thought', '')[:100] + '...') if len(node.state.get('thought', '')) > 100 else node.state.get('thought', '')
    action_preview = (node.state.get('action', '')[:80] + '...') if len(node.state.get('action', '')) > 80 else node.state.get('action', '')
    obs_preview = (node.state.get('observation', '')[:80] + '...') if len(node.state.get('observation', '')) > 80 else node.state.get('observation', '')
    
    uct_val = node.uct() if node.parent else 0.0
    
    detail = f"""
    â”Œâ”€ Node Details â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â”‚ Depth: {node.depth} | Visits: {node.visits} | Value: {node.value:.4f} | UCT: {uct_val:.4f}
    â”‚ Terminal: {node.is_terminal} | Reward: {node.reward} | EM: {node.em}
    â”‚ Children: {len(node.children)} | Exhausted: {node.exhausted}
    â”‚
    â”‚ Thought: {thought_preview}
    â”‚ Action:  {action_preview}
    â”‚ Obs:     {obs_preview}
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"""
    return detail

def log_tree_structure(root, max_depth=5):
    """
    íŠ¸ë¦¬ êµ¬ì¡°ë¥¼ ì‹œê°ì ìœ¼ë¡œ ë¡œê¹…í•©ë‹ˆë‹¤.
    
    Args:
        root: ë£¨íŠ¸ ë…¸ë“œ
        max_depth: ìµœëŒ€ ì¶œë ¥ ê¹Šì´
    """
    def _log_tree_recursive(node, level=0, prefix="", is_last=True):
        if node is None or level > max_depth:
            return
        
        # í˜„ì¬ ë…¸ë“œ ì •ë³´
        connector = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "
        node_info = f"Depth:{node.depth} V:{node.value:.2f} Visits:{node.visits} UCT:{node.uct():.2f}"
        if node.is_terminal:
            node_info += f" [TERMINAL R:{node.reward}]"
        
        logging.info(f"{prefix}{connector}{node_info}")
        
        # Thought/Action ë¯¸ë¦¬ë³´ê¸°
        if node.state.get('thought'):
            thought_short = node.state['thought'][:60] + '...' if len(node.state['thought']) > 60 else node.state['thought']
            logging.info(f"{prefix}{'    ' if is_last else 'â”‚   '}  â””â”€ Thought: {thought_short}")
        if node.state.get('action'):
            action_short = node.state['action'][:60] + '...' if len(node.state['action']) > 60 else node.state['action']
            logging.info(f"{prefix}{'    ' if is_last else 'â”‚   '}  â””â”€ Action: {action_short}")
        
        # ìì‹ ë…¸ë“œë“¤
        for i, child in enumerate(node.children):
            is_last_child = (i == len(node.children) - 1)
            extension = "    " if is_last else "â”‚   "
            _log_tree_recursive(child, level + 1, prefix + extension, is_last_child)
    
    logging.info("=" * 80)
    logging.info("TREE STRUCTURE:")
    logging.info("=" * 80)
    _log_tree_recursive(root, 0, "", True)
    logging.info("=" * 80)

def log_tree_statistics(root):
    """
    íŠ¸ë¦¬ì˜ í†µê³„ ì •ë³´ë¥¼ ë¡œê¹…í•©ë‹ˆë‹¤.
    
    Args:
        root: ë£¨íŠ¸ ë…¸ë“œ
    """
    all_nodes = collect_all_nodes(root)
    
    if not all_nodes:
        logging.info("Tree Statistics: No nodes found")
        return
    
    # ê¹Šì´ë³„ í†µê³„
    depth_stats = {}
    terminal_count = 0
    reward_1_count = 0
    total_visits = 0
    total_value = 0
    
    for node in all_nodes:
        depth = node.depth
        if depth not in depth_stats:
            depth_stats[depth] = {'count': 0, 'avg_value': 0, 'total_visits': 0}
        depth_stats[depth]['count'] += 1
        depth_stats[depth]['total_visits'] += node.visits
        total_visits += node.visits
        total_value += node.value
        
        if node.is_terminal:
            terminal_count += 1
            if node.reward == 1:
                reward_1_count += 1
    
    # í†µê³„ ë¡œê¹…
    logging.info("=" * 80)
    logging.info("TREE STATISTICS:")
    logging.info("=" * 80)
    logging.info(f"Total Nodes: {len(all_nodes)}")
    logging.info(f"Total Visits: {total_visits}")
    logging.info(f"Average Value: {total_value / len(all_nodes):.4f}" if all_nodes else "N/A")
    logging.info(f"Terminal Nodes: {terminal_count} ({reward_1_count} with reward=1)")
    logging.info(f"Max Depth: {max(depth_stats.keys()) if depth_stats else 0}")
    logging.info("")
    logging.info("Depth Distribution:")
    for depth in sorted(depth_stats.keys()):
        stats = depth_stats[depth]
        avg_value = stats['total_visits'] / stats['count'] if stats['count'] > 0 else 0
        logging.info(f"  Depth {depth}: {stats['count']} nodes, {stats['total_visits']} visits, avg_value: {avg_value:.4f}")
    logging.info("=" * 80)

def print_tree(node, level=0):
    """
    íŠ¸ë¦¬ êµ¬ì¡°ë¥¼ ë“¤ì—¬ì“°ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶œë ¥í•©ë‹ˆë‹¤.
    
    ë””ë²„ê¹… ëª©ì ìœ¼ë¡œ ë…¸ë“œì™€ ê·¸ ìì‹ë“¤ì„ ê³„ì¸µì ìœ¼ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.
    
    Args:
        node: ì¶œë ¥ì„ ì‹œì‘í•  ë…¸ë“œ
        level: í˜„ì¬ ê¹Šì´ (ë“¤ì—¬ì“°ê¸° ë ˆë²¨)
    """
    indent = "  " * level
    print(f"{indent}{node}")
    for child in node.children:
        print_tree(child, level + 1)

def backpropagate(node, value):
    """
    ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ë¥¼ ë¶€ëª¨ ë…¸ë“œë“¤ë¡œ ì—­ì „íŒŒí•©ë‹ˆë‹¤.
    
    MCTS ì•Œê³ ë¦¬ì¦˜ì˜ Backpropagation ë‹¨ê³„ì…ë‹ˆë‹¤. ì‹œë®¬ë ˆì´ì…˜ì—ì„œ ì–»ì€ ê°€ì¹˜ë¥¼
    ë£¨íŠ¸ ë…¸ë“œê¹Œì§€ ì˜¬ë¼ê°€ë©° ê° ë…¸ë“œì˜ visitsì™€ valueë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    ì¢…ë£Œ ë…¸ë“œì˜ ê²½ìš° ë³´ìƒì— ë”°ë¼ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ê°€ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    
    Args:
        node: ì—­ì „íŒŒë¥¼ ì‹œì‘í•  ë…¸ë“œ (ë³´í†µ ì‹œë®¬ë ˆì´ì…˜ì´ ì¢…ë£Œëœ ë…¸ë“œ)
        value: ì—­ì „íŒŒí•  ê°€ì¹˜ (ì‹œë®¬ë ˆì´ì…˜ì—ì„œ ì–»ì€ ë³´ìƒ)
    """
    # import pdb; pdb.set_trace()
    while node:
        node.visits += 1  # ë°©ë¬¸ íšŸìˆ˜ ì¦ê°€
        if node.is_terminal:
            # ì¢…ë£Œ ë…¸ë“œì˜ ê²½ìš°: ë³´ìƒì´ 0ì´ë©´ -1ì„, 1ì´ë©´ ì‹œë®¬ë ˆì´ì…˜ ê°€ì¹˜ë¥¼ ì‚¬ìš©
            if node.reward == 0:
                node.value = (node.value * (node.visits - 1) + (-1)) / node.visits
                logging.info(f"    Depth {node.depth}: Terminal (reward=0) â†’ value: {node.value:.4f} (visits: {node.visits})")
            else:
                node.value = (node.value * (node.visits - 1) + value) / node.visits
                logging.info(f"    Depth {node.depth}: Terminal (reward=1) â†’ value: {node.value:.4f} (visits: {node.visits})")
        else:
            # ë¹„ì¢…ë£Œ ë…¸ë“œ: ì‹œë®¬ë ˆì´ì…˜ ê°€ì¹˜ë¡œ ì—…ë°ì´íŠ¸
            node.value = (node.value * (node.visits - 1) + value) / node.visits
            logging.info(f"    Depth {node.depth}: Non-terminal â†’ value: {node.value:.4f} (visits: {node.visits})")

        node = node.parent  # ë¶€ëª¨ ë…¸ë“œë¡œ ì´ë™

def generate_prompt(node):
    """
    ë…¸ë“œì˜ í˜„ì¬ ìƒíƒœë¥¼ ê¸°ë°˜ìœ¼ë¡œ LLMì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    ë…¸ë“œì—ì„œ ë£¨íŠ¸ê¹Œì§€ì˜ ì „ì²´ ê¶¤ì (Thought, Action, Observation ì‹œí€€ìŠ¤)ì„
    ìˆ˜ì§‘í•˜ì—¬ ì§ˆë¬¸ê³¼ í•¨ê»˜ í•˜ë‚˜ì˜ í”„ë¡¬í”„íŠ¸ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.
    ì´ í”„ë¡¬í”„íŠ¸ëŠ” LLMì´ ë‹¤ìŒ ì•¡ì…˜ì„ ìƒì„±í•˜ê±°ë‚˜ í˜„ì¬ ìƒíƒœë¥¼ í‰ê°€í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
    
    Args:
        node: í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•  ë…¸ë“œ
    
    Returns:
        str: ì§ˆë¬¸ê³¼ ê¶¤ì ì„ í¬í•¨í•œ ì™„ì „í•œ í”„ë¡¬í”„íŠ¸
    """
    # import pdb; pdb.set_trace()
    trajectory = []
    question = node.question
    while node:
        new_segment = []
        if node.state['thought']:
            new_segment.append(f"Thought {node.depth}: {node.state['thought']}")
        if node.state['action']:
            new_segment.append(f"Action {node.depth}: {node.state['action']}")
        if node.state['observation'] and node.depth != 0:  # ë£¨íŠ¸ ë…¸ë“œì˜ observationì€ ì œì™¸
            new_segment.append(f"Observation {node.depth}: {node.state['observation']}")
        trajectory.append('\n'.join(new_segment))
        node = node.parent
    return question + '\n'.join(reversed(trajectory))  # ë£¨íŠ¸ë¶€í„° í˜„ì¬ ë…¸ë“œê¹Œì§€ì˜ ìˆœì„œë¡œ